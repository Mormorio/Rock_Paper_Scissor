{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839d563c",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5731a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pathlib\n",
    "import threading\n",
    "from queue import Queue\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "import tensorflow\n",
    "from torchvision.transforms import Resize, Lambda, Compose\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5108b9e",
   "metadata": {},
   "source": [
    "# Functions for face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2cb459",
   "metadata": {},
   "source": [
    "This function compute the percentage of overlapping of two squares, applied to bounding boxes helps keeping track of a single face through different frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdc5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_value(x,y,w,h,x1,y1,w1,h1):\n",
    "    a=min([x+w-x1, x1+w1-x])\n",
    "    b=min([y+h-y1, y1+h1-y])\n",
    "    if (a>0) & (b>0):\n",
    "        intersezione=a*b\n",
    "        score=intersezione/(h*w+h1*w1-intersezione)\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc0a49",
   "metadata": {},
   "source": [
    "With the following function we extract from the 68 landmarks computed by dlib the 5 landmarks used for performing the alinement in the training set of sphereface model, in the 5 landmarks model are required two landmarks for center of the eyes, in the 68 landmark model are provided instead only the contour of the eyes, we compute the center of the eye using the mean of the leftmost and rightmost landmarks of each eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e324e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_conversion(face_landmarks):\n",
    "    right_eye=[(face_landmarks.part(36).x+face_landmarks.part(39).x)/2,(face_landmarks.part(36).y+face_landmarks.part(39).y)/2]\n",
    "    left_eye=[(face_landmarks.part(42).x+face_landmarks.part(45).x)/2,(face_landmarks.part(42).y+face_landmarks.part(45).y)/2]\n",
    "    nose=[face_landmarks.part(30).x, face_landmarks.part(30).y]\n",
    "    mouth_right=[face_landmarks.part(48).x, face_landmarks.part(48).y]\n",
    "    mouth_left=[face_landmarks.part(54).x, face_landmarks.part(54).y]\n",
    "    landmarks=np.array([right_eye, left_eye, nose, mouth_right, mouth_left])\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875b355",
   "metadata": {},
   "source": [
    "Once we performed the landmark conversion we need a function that perform the alignement, this function gives as output the aligned foto of the correct size for sphereface model as well as an oversized one used by the game's scoreboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7ec30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment(foto, landmarks):\n",
    "    crop_size = (96, 112)\n",
    "    crop_size2 = (144, 144)\n",
    "    a=int((crop_size2[0]-crop_size[0])/2)\n",
    "    b=int((crop_size2[1]-crop_size[1])/2)\n",
    "\n",
    "    reference = [ [30.2946, 51.6963],[65.5318, 51.5014],[48.0252, 71.7366],[33.5493, 92.3655],[62.7299, 92.2041] ]\n",
    "    reference2=[]\n",
    "    for lm in reference:\n",
    "        reference2.append([lm[0]+a, lm[1]+b])\n",
    "\n",
    "    landmarks_ = np.array(landmarks, dtype=np.float32)\n",
    "    reference_ = np.array(reference2, dtype=np.float32)\n",
    "\n",
    "    transformation_matrix, _ = cv2.estimateAffinePartial2D(landmarks_, reference_)\n",
    "\n",
    "    foto_aligned_grossa = cv2.warpAffine(foto, transformation_matrix, crop_size2)\n",
    "    foto_aligned = foto_aligned_grossa[b:b+crop_size[1], a:a+crop_size[0] ,:]\n",
    "\n",
    "    return [foto_aligned, foto_aligned_grossa]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee55b9f",
   "metadata": {},
   "source": [
    "This function convert an image stored as a numpy array in a torch tensor where each value ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d154dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Compose([\n",
    "    Lambda(lambda x: torch.from_numpy(x)),\n",
    "    Lambda(lambda x: (x - 127.5 ) / 128.0), \n",
    "    Lambda(lambda x: x.permute(2,0,1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815723e",
   "metadata": {},
   "source": [
    "# Models for face processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b28691",
   "metadata": {},
   "source": [
    "Now we import the frontal face detector and the landmark position estimator from dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2ffea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = dlib.get_frontal_face_detector()\n",
    "dlib_facelandmark=dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a92772",
   "metadata": {},
   "source": [
    "we also need to import sphereface model to extract features from faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28562fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net_sphere import sphere20a\n",
    "model = sphere20a(feature=True)\n",
    "model.load_state_dict(torch.load(\"models/sphere20a_20171020.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4ae74",
   "metadata": {},
   "source": [
    "# Auxiliary class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe80c41",
   "metadata": {},
   "source": [
    "In this section we define two important classes: the first one represented an user and keep track of all the information related to the user, including a picture, the embeddings and the statistic of the game.\n",
    "\n",
    "The second class instead represent a face and contain information of following observations of the same face in consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184c9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utente:\n",
    "    def __init__(self, foto, colore, punti, lista_embeddings):\n",
    "        self.foto = foto\n",
    "        self.colore = colore #espresso come lista BGR\n",
    "        self.punti=punti\n",
    "        self.lista_embeddings=lista_embeddings\n",
    "        self.aggiorna_statistiche()\n",
    "\n",
    "    def aggiorna_statistiche(self):\n",
    "        self.mean_=np.mean(self.lista_embeddings, axis=0)\n",
    "        self.mean=self.mean_/np.linalg.norm(self.mean_)\n",
    "        \n",
    "class Faccia:\n",
    "    def __init__(self, x, y, w, h, id_utente, lista_embeddings):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        \n",
    "        self.id_utente=id_utente\n",
    "        self.id_faccia=id_utente  #inizialmente id_faccia e id_utente corrispondono e sono una cosa negativa\n",
    "        \n",
    "        self.rilevamento=1\n",
    "        self.reset_attesa()\n",
    "        \n",
    "        self.lista_embeddings=lista_embeddings\n",
    "        \n",
    "    def reset_attesa(self):\n",
    "        self.attesa=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3a129",
   "metadata": {},
   "source": [
    "# Main function for face detection and recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc6303",
   "metadata": {},
   "source": [
    "This is the more complex function used to perform face detection and user recognition, at first we run the frontal face detector, then for each face detected we compare it position to the faces detected in the previous frames, if there is exactly a face that was close enough we consider the new face as if it belong to the same user than the face detected before.\n",
    "\n",
    "Then if we do not have enough data related to that face we run the model to estimate landmarks position, then we align the image and we compute multidimensional face embeddings.\n",
    "\n",
    "Once we have computed the embedding we compute the mean of the embeddings related to that face (considering the ones collected in the previous frames) and we compute the cosine similiraty with the other user registered, if there is an user with cosine similarity higher than 0.7 we consider that face corresponding to the registered user.\n",
    "\n",
    "If there is no such user that match the detected face we will contine to take embedding untill we have 10, if the face still do not correspond to any user we use that embeddings as material to register a new user.\n",
    "\n",
    "At the end of the block there are also functions used to remove redundand faces (like faces that haven't be detected in a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8db312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_process(frame, face_list, utenti_registrati, id_, cycle_counter, result_queue):\n",
    "    \n",
    "    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces, scores, ids = face_detector.run(gray, 1, -1)\n",
    "    \n",
    "    for face in face_list:\n",
    "        face.rilevamento=0\n",
    "        \n",
    "    #for every face detected\n",
    "    for i in range(len(faces)):\n",
    "        if (ids[i]==0) & (scores[i]>0):\n",
    "            face_=faces[i]\n",
    "                \n",
    "            #computation of the bounding box\n",
    "            x=face_.left()\n",
    "            y=face_.top()\n",
    "            w=face_.right()-face_.left()\n",
    "            h=face_.bottom()-face_.top()\n",
    "\n",
    "            #check if there are corresponding faces in the list of old frame's faces\n",
    "            max_score=0\n",
    "            best_match=0\n",
    "            counter=0\n",
    "            for i in range(len(face_list)): #this compute the closeness of each face respect to the old ones registered\n",
    "                face=face_list[i]\n",
    "                score=score_value(x, y, w, h, face.x, face.y, face.w, face.h)\n",
    "                if score>0:\n",
    "                    counter=counter+1\n",
    "                if score>max_score:\n",
    "                    max_score=score\n",
    "                    best_match=i\n",
    "            \n",
    "            #if the face correspond to one and only one face in the old frames we consider as they are the same face\n",
    "            sovrapposizione_minima=0.1\n",
    "            if (max_score>sovrapposizione_minima) & (counter==1):\n",
    "                face=face_list[best_match]\n",
    "                face.rilevamento=1\n",
    "                face.x=x\n",
    "                face.y=y\n",
    "                face.h=h\n",
    "                face.w=w\n",
    "                \n",
    "                #if we do not have still identified the face we run the following code for face embeddings at every cycle, \n",
    "                #otherwise we run the following code once every 5 cycle\n",
    "                if ((face.id_utente<0) | ((cycle_counter%5)==0)):\n",
    "                    \n",
    "                    #detection of face landmarks                \n",
    "                    face_landmarks=dlib_facelandmark(gray, face_)                \n",
    "                    landmarks=landmarks_conversion(face_landmarks)\n",
    "\n",
    "                    #face alignement\n",
    "                    foto_=alignment(frame, landmarks)\n",
    "                    foto=foto_[1]\n",
    "\n",
    "                    #computation of embeddings using sphereface model\n",
    "                    emb = model(trans(foto_[0])[None])\n",
    "                    emb_=emb.detach().numpy()[0]\n",
    "\n",
    "                    #the frame embedding is added to the previous recorded ones\n",
    "                    face.lista_embeddings=np.vstack([face.lista_embeddings, emb_])\n",
    "\n",
    "                    #the belief on the user detected get updated, we check before if the last belief is confirmed and proceed checking\n",
    "                    #other users if this is not the case\n",
    "                    mean_face=np.mean(face.lista_embeddings, axis=0)\n",
    "                    mean_face=mean_face/np.linalg.norm(mean_face)\n",
    "                    v=0\n",
    "                    if face.id_utente>=0:\n",
    "                        user=utenti_registrati[face.id_utente]\n",
    "                        mean=user.mean\n",
    "                        v=np.dot(mean,mean_face)\n",
    "                    if v<0.7:\n",
    "                        old_belief=face.id_utente\n",
    "                        face.id_utente=face.id_faccia\n",
    "                        for i in range(len(utenti_registrati)):\n",
    "                            if i!=old_belief:\n",
    "                                user=utenti_registrati[i]\n",
    "                                mean=user.mean                        \n",
    "                                if np.dot(mean, mean_face)>0.7:\n",
    "                                    face.id_utente=i\n",
    "\n",
    "                    numero_dati=face.lista_embeddings.shape[0]\n",
    "                    \n",
    "                    #if we have reached a big number of data we use this data to create a new user if the face do not correspond\n",
    "                    #to a registered user, otherwise we will use the data to update/add more data to the user\n",
    "                    block_size=10\n",
    "                    if (numero_dati%block_size)==0:\n",
    "                        g=int(numero_dati/block_size)\n",
    "                        #if this is a new user i register it\n",
    "                        if face.id_utente<0:\n",
    "                            a=random.randint(0, 225)+30\n",
    "                            b=random.randint(0, 225)+30\n",
    "                            c=random.randint(0, 225)+30\n",
    "                            colore=[a,b,c]\n",
    "                            punti=[0,0,0]\n",
    "                            utenti_registrati.append(Utente(foto, colore, punti, face.lista_embeddings))\n",
    "                        #if the user is already registered i update informations adding new embeddings\n",
    "                        else:\n",
    "                            samples=face.lista_embeddings\n",
    "                            utenti_registrati[face.id_utente].lista_embeddings=np.vstack([face.lista_embeddings[(g-1)*block_size:,:], utenti_registrati[face.id_utente].lista_embeddings])\n",
    "                            utenti_registrati[face.id_utente].aggiorna_statistiche()\n",
    "                    \n",
    "            #if no old face (entity) correspond to the current face (detected) we create an additional face (entity)\n",
    "            else:\n",
    "                #detection of face landmarks                \n",
    "                face_landmarks=dlib_facelandmark(gray, face_)                \n",
    "                landmarks=landmarks_conversion(face_landmarks)\n",
    "                    \n",
    "                #face alignement\n",
    "                foto_=alignment(frame, landmarks)\n",
    "                foto=foto_[1]\n",
    "\n",
    "                #computation of embeddings using sphereface model\n",
    "                emb = model(trans(foto_[0])[None])\n",
    "                emb_=emb.detach().numpy()[0]\n",
    "                \n",
    "                face_list.append(Faccia(x,y,w,h,id_, emb_))\n",
    "                id_=id_-1\n",
    "                \n",
    "                \n",
    "            \n",
    "        #update face entity informations\n",
    "        for face in face_list:\n",
    "            if face.rilevamento==0:\n",
    "                face.attesa=face.attesa-1\n",
    "                face.permanenza=0\n",
    "            else:\n",
    "                face.reset_attesa()\n",
    "            \n",
    "        #I subtract to the face list all the faces that hasn't been detected in a while\n",
    "        new_face_list=[]\n",
    "        for face in face_list:\n",
    "            if face.attesa>0:\n",
    "                new_face_list.append(face)\n",
    "\n",
    "        #If there are multiple overlapping face entity we discard all the problematic faces\n",
    "        proximity=np.zeros(len(new_face_list))\n",
    "        for i in range(len(new_face_list)):\n",
    "            face=new_face_list[i]\n",
    "            for j in range(len(new_face_list)):\n",
    "                face2=new_face_list[j]\n",
    "                if i!=j:\n",
    "                    score=score_value(face.x, face.y, face.w, face.h, face2.x, face2.y, face2.w, face2.h)\n",
    "                    proximity[i]=max(score,proximity[i])\n",
    "        comb=zip(proximity, new_face_list)\n",
    "        face_list=[face for score, face in comb if score < 0.3]\n",
    "        \n",
    "        \n",
    "    result_queue.put([face_list, utenti_registrati, id_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd3d08",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# functions for gesture recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce5652",
   "metadata": {},
   "source": [
    "This block contains the two main functions that prepare the data (landmarks) obtained in the frame to be used in the gesture recognition model.\n",
    "\n",
    "Distance_map returns the list of normalized reciprocal distances of every possible pair of landmarks in the hand.\n",
    "\n",
    "Finger_angles returns the value of 10 angles selected among the landmarks. (Details in notebook RPS_Dataset.ipynb).\n",
    "\n",
    "Finally, the function prep simultaneously applies these two functions to the landmarks of a hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa7b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the list of the all possible couples of the landmarks\n",
    "def couples():\n",
    "    a = [i for i in range(21)]\n",
    "    b = []\n",
    "    for i in a:\n",
    "        for j in range(len(a)-(i+1)):\n",
    "            b.append(([i, i+j+1]))\n",
    "    return b\n",
    "\n",
    "#Euclidean distance\n",
    "def dist(p, q):\n",
    "    distance = math.sqrt((p[0] - q[0])**2 + (p[1] - q[1])**2)\n",
    "    return distance\n",
    "\n",
    "def distance_map(landmark_list):\n",
    "    couples_list = couples()\n",
    "    distances = []\n",
    "    \n",
    "    #Compute distance for each possible couple\n",
    "    for i in couples_list:\n",
    "        p = [landmark_list.landmark[i[0]].x, landmark_list.landmark[i[0]].y] #Point 1\n",
    "        q = [landmark_list.landmark[i[0]].x, landmark_list.landmark[i[1]].y] #Point 2\n",
    "        distance = dist(p, q)\n",
    "        distances.append(distance)\n",
    "\n",
    "    #Normalization\n",
    "    total_distance = sum(distances)/100\n",
    "    distances = [distance / total_distance for distance in distances]  #Normalization step\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34344718",
   "metadata": {},
   "source": [
    "# importing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca01fd",
   "metadata": {},
   "source": [
    "importing the gesture recognition model trained by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138ff5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importare modello gesture recognition\n",
    "model_gesture = tensorflow.keras.models.load_model(\"models/RPSmodel.h5\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8554e6",
   "metadata": {},
   "source": [
    "# additional functions used for gesture recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeef9d0",
   "metadata": {},
   "source": [
    "Hand_detection is the function that returns the landmarks (and the image with the landmarks drawn) given a frame. (Details in the notebook RPS_Dataset.ipynb).\n",
    "\n",
    "Ok_detector returns the list of probabilities that each recognized hand in the frame is in the \"ok\" position, the pose used as the start game signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54e4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_detection(photo):\n",
    "    with mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.25, max_num_hands=4) as hands:\n",
    "\n",
    "        image = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        #for drawing landmarks\n",
    "        #if results.multi_hand_landmarks:  #if hand is detected\n",
    "        #    for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "        #        mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    return image, results\n",
    "\n",
    "def ok_detector(hands): \n",
    "    likely_list = []\n",
    "    for h in hands:\n",
    "        d = np.asarray(distance_map(h))\n",
    "        likely_list.append(model_gesture(d[None, :])[0][3])\n",
    "\n",
    "    return likely_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eef1fe",
   "metadata": {},
   "source": [
    "\n",
    "The gamestart function is used to determine if the game can start, meaning if both players have their thumbs up. In this function, the hand used by each player for playing is saved. This information will be important to make the final classification (rock, paper, or scissors) more robust. The hand used to start the game will be considered. This prevents the model from inadvertently classifying the other hand of the player, which may coincidentally be in a pose resembling those specific to the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5149083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamestart(dots, hand):\n",
    "   \n",
    "    indices = np.argsort(ok_detector(dots))[-2:]  #Index of the 2 most likely fists\n",
    "            \n",
    "    point = 0\n",
    "    error = True\n",
    "    while error:\n",
    "        try:\n",
    "            if(dots[indices[0]].landmark[point].x>dots[indices[1]].landmark[point].x):\n",
    "                hand_pl1 = hand[indices[0]].classification[0].label \n",
    "                hand_pl2 = hand[indices[1]].classification[0].label\n",
    "            else:\n",
    "                hand_pl1 = hand[indices[1]].classification[0].label \n",
    "                hand_pl2 = hand[indices[0]].classification[0].label\n",
    "            error = False\n",
    "        except:\n",
    "            point += 1\n",
    "\n",
    "    #Return true if I detect 2 fists and if there is enough movement\n",
    "    if np.sum(np.asarray(ok_detector(dots)) > 0.5) >= 2 and np.sum(np.asarray(ok_detector(dots)) > 0.5) >= 2:\n",
    "        return True, hand_pl1, hand_pl2 \n",
    "    \n",
    "    return False, hand_pl1, hand_pl2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f426e7",
   "metadata": {},
   "source": [
    "This function returns the final classification of the hand poses of the players. As mentioned earlier, it only considers the hand used by each player to start the game. Additionally, it takes into account the positions of the hands in the scene to assign each hand to the correct player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7f4600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPS(dots, hands, hand_1, hand_2):\n",
    "    left=[0]\n",
    "    right=[0]\n",
    "    \n",
    "    point = 0\n",
    "    error = True\n",
    "    while error:\n",
    "        try:\n",
    "            mean_pos = np.mean([h.landmark[point].x for h in dots])  #Mean position of the hands\n",
    "\n",
    "            for i in range(len(dots)):   #iteration for hands detected\n",
    "                if dots[i].landmark[point].x < mean_pos: #Try to divide players\n",
    "                    if hands[i].classification[0].label == hand_2:\n",
    "                        d = np.asarray(distance_map(dots[i]))\n",
    "                        right = model_gesture(d[None, :])\n",
    "                else:\n",
    "                    if hands[i].classification[0].label == hand_1:\n",
    "                        d = np.asarray(distance_map(dots[i]))\n",
    "                        left = model_gesture(d[None, :])\n",
    "            error=False\n",
    "        except:\n",
    "            point += 1\n",
    "            print(point)\n",
    "    \n",
    "    return np.argmax(left), np.argmax(right), min([np.max(left), np.max(right)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221ce49",
   "metadata": {},
   "source": [
    "This function returns who has won given the moves made by the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02229f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner(a,b):\n",
    "    if a == (b+1)%3: \n",
    "        return -1\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da97ac",
   "metadata": {},
   "source": [
    "# importing user data and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402157d",
   "metadata": {},
   "source": [
    "This section is used to import images needed for the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de9b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_image = cv2.imread(\"immagini\\Scissor.jpeg\")\n",
    "p_image = cv2.imread(\"immagini\\Paper.jpeg\")\n",
    "r_image = cv2.imread(\"immagini\\Rock.jpeg\")\n",
    "\n",
    "winner_flag=cv2.imread(\"immagini\\winner.png\", cv2.IMREAD_UNCHANGED)\n",
    "winner_flag=cv2.resize(winner_flag,(200,100))\n",
    "\n",
    "image_list=[cv2.flip(r_image, 1), cv2.flip(p_image, 1), cv2.flip(s_image, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d4e96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_list=[]\n",
    "utenti_registrati=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442e3d9",
   "metadata": {},
   "source": [
    "This part is used to import data of users registered in old games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e24fd022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for user in os.listdir('users_data/embeddings'):\n",
    "\n",
    "  with open('users_data/embeddings/'+user, 'rb') as file:\n",
    "    lista_embeddings = pickle.load(file)\n",
    "  with open('users_data/colori/'+user, 'rb') as file:\n",
    "    colore = pickle.load(file)\n",
    "  with open('users_data/foto/'+user, 'rb') as file:\n",
    "    foto = pickle.load(file)\n",
    "  utenti_registrati.append(Utente(foto, colore, [0,0,0], lista_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b7940",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b387bbb",
   "metadata": {},
   "source": [
    "For what concern the main function of face detection and recognition is handled using a thread this way the frame rate of the camera is not made slower by that.\n",
    "\n",
    "The game is handled through a 4 state system, when the game is in the state 0 it wait for the signal to start the game (both player have to do the OK gesture), if the signal is recived the game switchs to state 1 where a countdown is displayed and at the end of that the simbols displayed by the users are classified. In the phase 2 scores are updated and then in phase 3 the result and the gesture detected are displayed on screen for 3 seconds, then the state automatically reset to 0.\n",
    "\n",
    "The gesture identified is associated to a specific user according to the position in the screen, we assume that during the game only 2 players are present on screen and the rightmost hand is associated with the rightmost user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39fe9283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pickle\\nimport os\\nimport torch'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim1=80\n",
    "dim2=20\n",
    "sp=2\n",
    "id_=-1\n",
    "cycle_counter=0\n",
    "\n",
    "classes = [\"Rock\", \"Paper\", \"Scissors\", \"Ok\"]\n",
    "start = False\n",
    "finish = False\n",
    "ti = None \n",
    "to_be_reported = False\n",
    "gamestate=0\n",
    "\n",
    "start_time = time.time()\n",
    "occupied = 0\n",
    "\n",
    "result_queue = Queue()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "        \n",
    "        #frame acquisition\n",
    "        ret, frame = cap.read()\n",
    "        frame2=copy.copy(frame)\n",
    "        \n",
    "        #The next block of code handle the thread for face detection and user identification procedures\n",
    "        face_list_2=[]\n",
    "        for face in face_list:\n",
    "            face_list_2.append(copy.copy(face))  \n",
    "        if occupied==0:\n",
    "            processing_thread = threading.Thread(target=detect_and_process, args=(frame,face_list_2,utenti_registrati,id_,cycle_counter,result_queue))\n",
    "            processing_thread.start()\n",
    "            occupied=1  \n",
    "        if result_queue.qsize()>0:\n",
    "            new_data=result_queue.get()\n",
    "            face_list=new_data[0]\n",
    "            utenti_registrati=new_data[1]\n",
    "            id_=new_data[2]\n",
    "            occupied=0\n",
    "            cycle_counter=cycle_counter+1\n",
    "            \n",
    "        #here we detect the order of players id in the image, this way we can associate each simbols to an user\n",
    "        players=[]\n",
    "        for face in face_list:\n",
    "            if (face.rilevamento==1) and (face.id_utente>=0):\n",
    "                players.append((face.x,face.id_utente))\n",
    "        sorted_players=sorted(players, key=lambda x: x[0])\n",
    "\n",
    "        \n",
    "        #hand detection\n",
    "        frame2, results = hand_detection(frame2)\n",
    "        dots = results.multi_hand_landmarks\n",
    "        hands = results.multi_handedness \n",
    "        \n",
    "        #Game's state main routine\n",
    "        #this part of the code is the main part that take care of game states\n",
    "        text = \" \"\n",
    "        if gamestate==0: #in state 0 we check if the users are doign the \"start signal\"\n",
    "            if (dots is not None) and (len(dots)>1) and (len(players)==2):\n",
    "                game_players=players.copy()\n",
    "                start, hand_1, hand_2 = gamestart(dots, hands)\n",
    "                if start:\n",
    "                    ti = time.time()\n",
    "                    gamestate+=1\n",
    "        if gamestate==1: #in gamestate 1 we display the countdown and classify the simbols\n",
    "            text=str(max(int(ti+3.9-time.time()),0))\n",
    "            if ((time.time()-ti)>=5) and (dots is not None) and (len(dots)>1):\n",
    "                out1, out2, confidence = RPS(dots, hands, hand_1, hand_2)\n",
    "                to=time.time()\n",
    "                #print(str(out1)+\" \"+str(out2)+\" \"+str(confidence))\n",
    "                if (confidence>0.60) and (out1!=3) and (out2!=3):\n",
    "                    gamestate+=1\n",
    "                    to_be_reported=True\n",
    "        if gamestate == 2: #in gamestate 2 scores are updated\n",
    "            text=\" \"\n",
    "            if to_be_reported:\n",
    "                win = winner(out1, out2)\n",
    "                if win==-1:\n",
    "                    utenti_registrati[game_players[1][1]].punti[2]+=1\n",
    "                    utenti_registrati[game_players[0][1]].punti[0]+=1\n",
    "                    j=1\n",
    "                elif win==0:\n",
    "                    utenti_registrati[game_players[1][1]].punti[1]+=1\n",
    "                    utenti_registrati[game_players[0][1]].punti[1]+=1\n",
    "                    j=0\n",
    "                else:\n",
    "                    utenti_registrati[game_players[1][1]].punti[0]+=1\n",
    "                    utenti_registrati[game_players[0][1]].punti[2]+=1\n",
    "                    j=-1\n",
    "                to_be_reported=False\n",
    "                gamestate += 1\n",
    "                ti=time.time()\n",
    "        if gamestate==3 and (time.time() - ti <= 3): #in gamestate 3 results are displayed on screen\n",
    "            if j!=0:\n",
    "                for d1 in range(winner_flag.shape[0]):\n",
    "                    for d2 in range(winner_flag.shape[1]):\n",
    "                        if winner_flag[d1,200-d2-1,3]>0.5:\n",
    "                            frame2[0+d1, 220+100*j+d2]=winner_flag[d1,200-d2-1,:3]\n",
    "            frame2[65:165, 170:270] = cv2.resize(image_list[out2], (100,100))\n",
    "            frame2[65:165, 370:470] = cv2.resize(image_list[out1], (100,100))  \n",
    "        if gamestate==3 and (time.time() - ti >= 3): #here the game state is setted again to 0 to start a new game\n",
    "            gamestate=0\n",
    "        \n",
    "        #Next blocks of code are related to the visual part of the game\n",
    "        #display face bounding box\n",
    "        for face in face_list:\n",
    "            if face.rilevamento==1:\n",
    "                if face.id_utente>=0:\n",
    "                    colori=utenti_registrati[face.id_utente].colore\n",
    "                else:\n",
    "                    colori=[0,0,0]\n",
    "                cv2.rectangle(frame2, (face.x,face.y), (face.x+face.w,face.y+face.h), (colori[0], colori[1], colori[2]), 2)\n",
    "        frame2 = cv2.flip(frame2, 1)\n",
    "        pad=np.ones((dim2, dim1+sp, 1))\n",
    "        \n",
    "        #update scoreboard\n",
    "        for i in range(len(utenti_registrati)):\n",
    "            try:\n",
    "                colori=utenti_registrati[i].colore\n",
    "                punti=utenti_registrati[i].punti\n",
    "                \n",
    "                cv2.rectangle(frame2, (sp,i*(dim1+dim2)+sp), (dim1-sp,i*(dim1+dim2)+dim1-sp), (colori[0], colori[1], colori[2]), 2*sp)\n",
    "                frame2[i*(dim1+dim2)+dim1:(i+1)*(dim1+dim2),0:dim1+sp]=np.concatenate((pad*colori[0],pad*colori[1],pad*colori[2]),axis=2)\n",
    "                frame2[i*(dim1+dim2)+2*sp:i*(dim1+dim2)+dim1-2*sp,2*sp:dim1-2*sp]=cv2.resize(utenti_registrati[i].foto, (dim1-4*sp,dim1-4*sp))\n",
    "                testo = str(punti[0])+\" \"+str(punti[1])+\" \"+str(punti[2])\n",
    "                posizione_testo = (15, (dim1+dim2)*i+dim1+12)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                scala_font = 0.5\n",
    "                colore_font = (0, 0, 0)\n",
    "                spessore_linea = 1\n",
    "                cv2.putText(frame2, testo, posizione_testo, font, scala_font, colore_font, spessore_linea)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        #add text to the frame\n",
    "        cv2.putText(frame2, text, (310, 70), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)\n",
    "        \n",
    "            \n",
    "        cv2.imshow('Window', frame2)\n",
    "        \n",
    "        key = cv2.waitKey(10)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47af108",
   "metadata": {},
   "source": [
    "# Saving users data for next rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19d7ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os\\nimport torch\\nimport pickle'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving users data\n",
    "for i in range(len(utenti_registrati)):\n",
    "    utente=utenti_registrati[i]\n",
    "    numero_embeddings=utente.lista_embeddings.shape[0]\n",
    "    numero_massimo=50\n",
    "    with open('users_data/embeddings'+'/user_'+str(i), \"wb\") as file:\n",
    "        if numero_embeddings<numero_massimo:\n",
    "            pickle.dump(utente.lista_embeddings, file)\n",
    "        else:\n",
    "            r=random.sample(range(1, numero_embeddings), 50)\n",
    "            pickle.dump(utente.lista_embeddings[r,:], file)\n",
    "        \n",
    "    with open('users_data/colori'+'/user_'+str(i), \"wb\") as file:\n",
    "        pickle.dump(utente.colore, file)\n",
    "    with open('users_data/foto'+'/user_'+str(i), \"wb\") as file:\n",
    "        pickle.dump(utente.foto, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
